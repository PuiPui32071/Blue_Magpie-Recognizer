{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating mean and std: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:14<00:00,  9.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import get_mean_std, get_label_map\n",
    "\n",
    "mean, std = get_mean_std('dataset_1500')\n",
    "label_map = get_label_map('dataset_1500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "def process_image(image):\n",
    "    return transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=True)\n",
      "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1280, out_features=128, bias=True)\n",
      "  (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model = models.efficientnet_v2_s()\n",
    "print(model.classifier)\n",
    "\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    torch.nn.Linear(in_features, 128),\n",
    "    torch.nn.BatchNorm1d(128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(128, 3)\n",
    ")\n",
    "print(model.classifier)\n",
    "\n",
    "model.load_state_dict(torch.load('ckpts/effv2s_bn_0.001_10_0.5/best_val_loss.pth', map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    transformed_image = process_image(image)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(transformed_image.to(device))\n",
    "        probabilities = F.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "        return {label_map[i]: float(probabilities[i]) for i in range(len(label_map))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Occlusion Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    import io\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    img = Image.open(buf)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occlusion_image(image):\n",
    "\n",
    "    transformed_image = process_image(image)\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(transformed_image.to(device))\n",
    "    output = F.softmax(output, dim=1)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    pred.squeeze_()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    occlusion = Occlusion(model)\n",
    "\n",
    "    attributions_occ = occlusion.attribute(transformed_image.to(device),\n",
    "                                        strides = (3, 8, 8),\n",
    "                                        target=pred,\n",
    "                                        sliding_window_shapes=(3, 15, 15),\n",
    "                                        baselines=0)\n",
    "\n",
    "    figure, _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                                  np.array(image.resize((256, 256))),\n",
    "                                                  [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n",
    "                                                  [\"all\", \"positive\", \"positive\"],\n",
    "                                                  fig_size=(15, 5),\n",
    "                                                  alpha_overlay=0.7,\n",
    "                                                  show_colorbar=True,\n",
    "                                                  outlier_perc=2)\n",
    "\n",
    "    captum_image = fig2img(figure)\n",
    "    return captum_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_image():\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=(\n",
    "    \"\"\"\n",
    "    <center>\n",
    "        <h1> Blue Magpie Recognizer ðŸ¦œ </h1>\n",
    "        <b> This model recognizes the species of a blue magpie from an image. <b>\n",
    "    </center>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://7bc71d70e302cf48a9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7bc71d70e302cf48a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bmr3/lib/python3.10/site-packages/captum/attr/_utils/visualization.py:443: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/opt/anaconda3/envs/bmr3/lib/python3.10/site-packages/captum/attr/_utils/visualization.py:443: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(theme=\"soft\") as demo:\n",
    "    gr.Markdown(title)\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
    "            with gr.Row():\n",
    "                clear_button = gr.Button(\"Clear\")\n",
    "                upload_button = gr.Button(\"Submit\")\n",
    "        with gr.Column():\n",
    "            predict_output = gr.Label(num_top_classes=3, label=\"Prediction\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        captum_output = gr.Image(type=\"pil\", label=\"Captum heatmap\")\n",
    "\n",
    "    \n",
    "    upload_button.click(predict, inputs=image_input, outputs=predict_output)\n",
    "    upload_button.click(get_occlusion_image, inputs=image_input, outputs=captum_output)\n",
    "\n",
    "    clear_button.click(clear_image, inputs=[], outputs=[image_input, predict_output, captum_output])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmr3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
