{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating mean and std: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:10<00:00,  9.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import get_mean_std, get_label_map\n",
    "\n",
    "split_ratio = [0.7, 0.15, 0.15]\n",
    "mean, std = get_mean_std('dataset_1500', split_ratio=split_ratio, random_seed=2024)\n",
    "\n",
    "label_map = get_label_map('dataset_1500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "def process_image(image):\n",
    "    return transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model = models.efficientnet_v2_s()\n",
    "print(model.classifier)\n",
    "\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    torch.nn.Linear(in_features, 128),\n",
    "    torch.nn.BatchNorm1d(128),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(128, 3)\n",
    ")\n",
    "print(model.classifier)\n",
    "\n",
    "model.load_state_dict(torch.load('ckpts/effv2s_bn_si_0.001_10_0.5/best_val_acc.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(image):\n",
    "    transformed_image = process_image(image)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(transformed_image.to(device))\n",
    "        probs = F.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "        return {label_map[i]: prob for i, prob in enumerate(probs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Occlusion Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occlusion_image(image):\n",
    "\n",
    "    transformed_image = process_image(image)\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(transformed_image.to(device))\n",
    "    output = F.softmax(output, dim=1)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    pred.squeeze_()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    occlusion = Occlusion(model)\n",
    "\n",
    "    attributions_occ = occlusion.attribute(transformed_image.to(device),\n",
    "                                           strides = (3, 8, 8),\n",
    "                                           target=pred,\n",
    "                                           sliding_window_shapes=(3, 15, 15),\n",
    "                                           baselines=0)\n",
    "\n",
    "    fig, ax = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                                np.array(image.resize((256, 256))),\n",
    "                                                [\"original_image\", \"blended_heat_map\", \"heat_map\"],\n",
    "                                                [\"all\", \"positive\", \"positive\"],\n",
    "                                                fig_size=(15, 5),\n",
    "                                                alpha_overlay=0.7,\n",
    "                                                show_colorbar=True,\n",
    "                                                outlier_perc=2)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_image():\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=(\n",
    "    \"\"\"\n",
    "    <center>\n",
    "        <h1> Blue Magpie Recognizer ðŸ¦œ </h1>\n",
    "        <b> Upload an image and get the prediction! </b>\n",
    "    </center>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://0618e88c156a931009.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0618e88c156a931009.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bmr/lib/python3.10/site-packages/captum/attr/_utils/visualization.py:443: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(theme=\"soft\") as app:\n",
    "    gr.Markdown(title)\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_button = gr.Button(\"Clear\")\n",
    "                predict_button = gr.Button(\"Predict\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            predict_output = gr.Label(num_top_classes=3, label=\"Prediction\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        captum_output = gr.Plot(label=\"Occlusion Heat Map\")\n",
    "\n",
    "    \n",
    "    predict_button.click(get_prediction, inputs=image_input, outputs=predict_output)\n",
    "    predict_button.click(get_occlusion_image, inputs=image_input, outputs=captum_output)\n",
    "\n",
    "    clear_button.click(clear_image, inputs=[], outputs=[image_input, predict_output, captum_output])\n",
    "\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
